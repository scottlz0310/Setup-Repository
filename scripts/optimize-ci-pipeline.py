#!/usr/bin/env python3
"""
CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œæ™‚é–“ã‚’åˆ†æã—ã€æœ€é©åŒ–ææ¡ˆã‚’è¡Œã„ã¾ã™ã€‚
"""

import json
import os
import subprocess
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any


@dataclass
class PipelineMetrics:
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    stage_name: str
    start_time: float
    end_time: float
    duration: float
    success: bool
    cache_hit: bool = False
    parallel_workers: int = 1


class CIPipelineOptimizer:
    """CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.metrics: list[PipelineMetrics] = []
        self.cache_strategies = {
            "dependencies": ["~/.cache/uv", ".venv"],
            "test_cache": [".pytest_cache", ".mypy_cache", ".ruff_cache"],
            "build_cache": ["dist/", "build/"],
        }

    def measure_stage(self, stage_name: str, command: list[str], parallel_workers: int = 1) -> PipelineMetrics:
        """ã‚¹ãƒ†ãƒ¼ã‚¸ã®å®Ÿè¡Œæ™‚é–“ã‚’æ¸¬å®š"""
        print(f"ğŸ”§ å®Ÿè¡Œä¸­: {stage_name}")

        start_time = time.perf_counter()

        try:
            result = subprocess.run(
                command,
                capture_output=True,
                text=True,
                timeout=1800,  # 30åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            )
            success = result.returncode == 0

            if not success:
                print(f"âŒ {stage_name} å¤±æ•—:")
                print(result.stderr)

        except subprocess.TimeoutExpired:
            print(f"â° {stage_name} ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            success = False
        except Exception as e:
            print(f"âŒ {stage_name} ã‚¨ãƒ©ãƒ¼: {e}")
            success = False

        end_time = time.perf_counter()
        duration = end_time - start_time

        metrics = PipelineMetrics(
            stage_name=stage_name,
            start_time=start_time,
            end_time=end_time,
            duration=duration,
            success=success,
            parallel_workers=parallel_workers,
        )

        self.metrics.append(metrics)

        status = "âœ…" if success else "âŒ"
        print(f"{status} {stage_name} å®Œäº† ({duration:.2f}ç§’)")

        return metrics

    def check_cache_effectiveness(self) -> dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®åŠ¹æœã‚’åˆ†æ"""
        cache_analysis = {}

        for cache_type, paths in self.cache_strategies.items():
            cache_info = {"exists": [], "sizes": [], "total_size_mb": 0}

            for path in paths:
                path_obj = Path(path)
                if path_obj.exists():
                    cache_info["exists"].append(str(path))

                    if path_obj.is_file():
                        size = path_obj.stat().st_size
                        cache_info["sizes"].append(size)
                        cache_info["total_size_mb"] += size / 1024 / 1024
                    elif path_obj.is_dir():
                        total_size = sum(f.stat().st_size for f in path_obj.rglob("*") if f.is_file())
                        cache_info["sizes"].append(total_size)
                        cache_info["total_size_mb"] += total_size / 1024 / 1024

            cache_analysis[cache_type] = cache_info

        return cache_analysis

    def analyze_parallel_efficiency(self) -> dict[str, Any]:
        """ä¸¦åˆ—å‡¦ç†ã®åŠ¹ç‡ã‚’åˆ†æ"""
        parallel_stages = [m for m in self.metrics if m.parallel_workers > 1]

        if not parallel_stages:
            return {"message": "ä¸¦åˆ—å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}

        analysis = {
            "parallel_stages": len(parallel_stages),
            "average_workers": sum(m.parallel_workers for m in parallel_stages) / len(parallel_stages),
            "total_parallel_time": sum(m.duration for m in parallel_stages),
            "recommendations": [],
        }

        # ä¸¦åˆ—å‡¦ç†ã®åŠ¹ç‡æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        for stage in parallel_stages:
            if stage.duration > 300:  # 5åˆ†ä»¥ä¸Š
                analysis["recommendations"].append(f"{stage.stage_name}: å®Ÿè¡Œæ™‚é–“ãŒé•·ã„ãŸã‚ã€ã‚ˆã‚Šå¤šãã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’æ¤œè¨")
            elif stage.parallel_workers > 8:
                analysis["recommendations"].append(f"{stage.stage_name}: ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ãŒå¤šã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")

        return analysis

    def generate_optimization_report(self) -> dict[str, Any]:
        """æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        total_duration = sum(m.duration for m in self.metrics)
        successful_stages = [m for m in self.metrics if m.success]
        failed_stages = [m for m in self.metrics if not m.success]

        # æœ€ã‚‚æ™‚é–“ã®ã‹ã‹ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’ç‰¹å®š
        slowest_stages = sorted(self.metrics, key=lambda x: x.duration, reverse=True)[:3]

        cache_analysis = self.check_cache_effectiveness()
        parallel_analysis = self.analyze_parallel_efficiency()

        report = {
            "summary": {
                "total_duration": total_duration,
                "successful_stages": len(successful_stages),
                "failed_stages": len(failed_stages),
                "total_stages": len(self.metrics),
            },
            "slowest_stages": [
                {
                    "name": stage.stage_name,
                    "duration": stage.duration,
                    "workers": stage.parallel_workers,
                }
                for stage in slowest_stages
            ],
            "cache_analysis": cache_analysis,
            "parallel_analysis": parallel_analysis,
            "recommendations": self._generate_recommendations(),
        }

        return report

    def _generate_recommendations(self) -> list[str]:
        """æœ€é©åŒ–æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []

        # å®Ÿè¡Œæ™‚é–“ãƒ™ãƒ¼ã‚¹ã®æ¨å¥¨äº‹é …
        long_stages = [m for m in self.metrics if m.duration > 180]  # 3åˆ†ä»¥ä¸Š
        if long_stages:
            recommendations.append(f"é•·æ™‚é–“å®Ÿè¡Œã‚¹ãƒ†ãƒ¼ã‚¸ ({len(long_stages)}å€‹) ã®ä¸¦åˆ—åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")

        # å¤±æ•—ã‚¹ãƒ†ãƒ¼ã‚¸ã®æ¨å¥¨äº‹é …
        failed_stages = [m for m in self.metrics if not m.success]
        if failed_stages:
            recommendations.append(f"å¤±æ•—ã‚¹ãƒ†ãƒ¼ã‚¸ ({len(failed_stages)}å€‹) ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’æ”¹å–„ã—ã¦ãã ã•ã„")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ¨å¥¨äº‹é …
        cache_analysis = self.check_cache_effectiveness()
        for cache_type, info in cache_analysis.items():
            if not info["exists"]:
                recommendations.append(f"{cache_type} ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            elif info["total_size_mb"] > 1000:  # 1GBä»¥ä¸Š
                recommendations.append(f"{cache_type} ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™ ({info['total_size_mb']:.1f}MB)")

        return recommendations

    def save_report(self, output_file: str = "ci-optimization-report.json") -> None:
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        report = self.generate_optimization_report()

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“Š æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")


def run_optimized_pipeline():
    """æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ"""
    optimizer = CIPipelineOptimizer()

    # ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    optimizer.measure_stage("ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«", ["uv", "sync", "--dev"])

    # ãƒªãƒ³ãƒ†ã‚£ãƒ³ã‚°ï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰
    optimizer.measure_stage(
        "Ruff ãƒªãƒ³ãƒ†ã‚£ãƒ³ã‚°",
        ["uv", "run", "ruff", "check", ".", "--output-format=json"],
        parallel_workers=1,
    )

    # å‹ãƒã‚§ãƒƒã‚¯
    optimizer.measure_stage("MyPy å‹ãƒã‚§ãƒƒã‚¯", ["uv", "run", "mypy", "src/"])

    # ä¸¦åˆ—ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    cpu_count = os.cpu_count() or 4
    test_workers = max(1, int(cpu_count * 0.75))

    optimizer.measure_stage(
        "ä¸¦åˆ—ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ",
        [
            "uv",
            "run",
            "pytest",
            "tests/",
            "-n",
            str(test_workers),
            "--dist",
            "worksteal",
            "--cov=src/setup_repo",
            "--cov-report=xml",
            "--junit-xml=test-results.xml",
        ],
        parallel_workers=test_workers,
    )

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆæ¡ä»¶ä»˜ãï¼‰
    if os.environ.get("RUN_PERFORMANCE_TESTS", "false").lower() == "true":
        optimizer.measure_stage(
            "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ",
            [
                "uv",
                "run",
                "pytest",
                "tests/performance/",
                "-m",
                "performance and not slow",
                "--json-report",
                "--json-report-file=performance-report.json",
            ],
        )

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    optimizer.save_report()

    # çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    report = optimizer.generate_optimization_report()

    print("\n" + "=" * 60)
    print("ğŸ“Š CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 60)

    summary = report["summary"]
    print(f"ç·å®Ÿè¡Œæ™‚é–“: {summary['total_duration']:.2f}ç§’")
    print(f"æˆåŠŸã‚¹ãƒ†ãƒ¼ã‚¸: {summary['successful_stages']}/{summary['total_stages']}")

    if report["recommendations"]:
        print("\nğŸ”§ æœ€é©åŒ–æ¨å¥¨äº‹é …:")
        for i, rec in enumerate(report["recommendations"], 1):
            print(f"  {i}. {rec}")

    print("\næœ€ã‚‚æ™‚é–“ã®ã‹ã‹ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸:")
    for stage in report["slowest_stages"]:
        print(f"  - {stage['name']}: {stage['duration']:.2f}ç§’ (ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {stage['workers']})")

    # å¤±æ•—ãŒã‚ã£ãŸå ´åˆã¯çµ‚äº†ã‚³ãƒ¼ãƒ‰1ã‚’è¿”ã™
    if summary["failed_stages"] > 0:
        print(f"\nâŒ {summary['failed_stages']}å€‹ã®ã‚¹ãƒ†ãƒ¼ã‚¸ãŒå¤±æ•—ã—ã¾ã—ãŸ")
        return 1

    print("\nâœ… å…¨ã‚¹ãƒ†ãƒ¼ã‚¸ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
    return 0


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    if len(sys.argv) > 1 and sys.argv[1] == "--analyze-only":
        # æ—¢å­˜ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’åˆ†æã®ã¿
        report_file = "ci-optimization-report.json"
        if Path(report_file).exists():
            with open(report_file, encoding="utf-8") as f:
                report = json.load(f)

            print("ğŸ“Š æ—¢å­˜ã®æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆåˆ†æ:")
            print(json.dumps(report, indent=2, ensure_ascii=False))
        else:
            print(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {report_file}")
            return 1
    else:
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
        return run_optimized_pipeline()


if __name__ == "__main__":
    sys.exit(main())
