name: Quality Metrics Collection

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  schedule:
    # 毎日午前2時（UTC）に実行
    - cron: "0 2 * * *"
  workflow_dispatch:

jobs:
  collect-metrics:
    name: Collect Quality Metrics
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # 履歴比較のため全履歴を取得

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install 3.11

      - name: Create virtual environment
        run: uv venv --python 3.11

      - name: Install dependencies
        run: uv sync --dev

      - name: Run quality analysis
        run: |
          echo "🔍 品質メトリクス収集中..."

          # カバレッジ監視システムでレポート生成
          uv run python scripts/coverage-monitor.py --generate-report

          # Ruffによるコード品質チェック
          echo "📝 Ruffリンティング実行中..."
          if ! uv run ruff check . --output-format=json > ruff-report.json; then
            echo "❌ Ruffリンティングでエラーが発生しました"
            exit 1
          fi

          # MyPyによる型チェック
          echo "🔍 MyPy型チェック実行中..."
          if ! uv run mypy src/ --json-report mypy-report.json; then
            echo "❌ MyPy型チェックでエラーが発生しました"
            exit 1
          fi

          # テスト実行とメトリクス収集
          echo "🧪 テスト実行中..."
          if ! uv run pytest tests/ \
            --json-report --json-report-file=test-report.json \
            --cov=src/setup_repo \
            --cov-report=json:coverage.json \
            -v; then
            echo "❌ テストが失敗しました"
            exit 1
          fi

      - name: Generate quality metrics summary
        run: |
          echo "📊 品質メトリクスサマリー生成中..."

          # 品質メトリクスサマリーを生成
          uv run python -c "
          import json
          import os
          from datetime import datetime
          from pathlib import Path

          # メトリクスデータを収集
          metrics = {
              'timestamp': datetime.now().isoformat(),
              'commit_sha': os.environ.get('GITHUB_SHA', 'unknown'),
              'branch': os.environ.get('GITHUB_REF_NAME', 'unknown'),
              'ruff_issues': 0,
              'mypy_errors': 0,
              'test_passed': 0,
              'test_failed': 0,
              'test_coverage': 0.0
          }

          # Ruffレポートを解析
          try:
              with open('ruff-report.json', 'r') as f:
                  ruff_data = json.load(f)
              metrics['ruff_issues'] = len(ruff_data)
          except:
              pass

          # MyPyレポートを解析
          try:
              with open('mypy-report.json', 'r') as f:
                  mypy_data = json.load(f)
              metrics['mypy_errors'] = len(mypy_data.get('errors', []))
          except:
              pass

          # テストレポートを解析
          try:
              with open('test-report.json', 'r') as f:
                  test_data = json.load(f)
              summary = test_data.get('summary', {})
              metrics['test_passed'] = summary.get('passed', 0)
              metrics['test_failed'] = summary.get('failed', 0)
          except:
              pass

          # カバレッジデータを解析
          try:
              with open('coverage.json', 'r') as f:
                  cov_data = json.load(f)
              metrics['test_coverage'] = cov_data['totals']['percent_covered']
          except:
              pass

          # メトリクスファイルを保存
          with open('quality-metrics.json', 'w') as f:
              json.dump(metrics, f, indent=2)

          # サマリーをMarkdown形式で生成
          with open('quality-summary.md', 'w') as f:
              f.write('# 📊 品質メトリクスサマリー\\n\\n')
              f.write(f'**実行日時**: {metrics[\"timestamp\"]}\\n')
              f.write(f'**コミット**: {metrics[\"commit_sha\"][:8]}\\n')
              f.write(f'**ブランチ**: {metrics[\"branch\"]}\\n\\n')

              f.write('## メトリクス\\n\\n')
              f.write(f'- **テストカバレッジ**: {metrics[\"test_coverage\"]:.2f}%\\n')
              f.write(f'- **テスト結果**: {metrics[\"test_passed\"]} 成功, {metrics[\"test_failed\"]} 失敗\\n')
              f.write(f'- **Ruff問題**: {metrics[\"ruff_issues\"]} 件\\n')
              f.write(f'- **MyPyエラー**: {metrics[\"mypy_errors\"]} 件\\n\\n')

              # 品質スコアを計算
              quality_score = 100
              if metrics['test_coverage'] < 80:
                  quality_score -= (80 - metrics['test_coverage']) * 2
              quality_score -= metrics['ruff_issues'] * 0.5
              quality_score -= metrics['mypy_errors'] * 1
              quality_score -= metrics['test_failed'] * 5
              quality_score = max(0, quality_score)

              f.write(f'## 品質スコア: {quality_score:.1f}/100\\n\\n')

              if quality_score >= 90:
                  f.write('✅ **優秀**: 品質基準を大幅に上回っています\\n')
              elif quality_score >= 80:
                  f.write('✅ **良好**: 品質基準を満たしています\\n')
              elif quality_score >= 70:
                  f.write('⚠️ **注意**: 品質改善が推奨されます\\n')
              else:
                  f.write('❌ **要改善**: 品質基準を下回っています\\n')

          print('品質メトリクスサマリーを生成しました')
          "

      - name: Store metrics history
        run: |
          echo "📈 メトリクス履歴を保存中..."

          # メトリクス履歴ディレクトリを作成
          mkdir -p quality-history

          # 日付付きファイル名でメトリクスを保存
          DATE=$(date +%Y%m%d_%H%M%S)
          cp quality-metrics.json quality-history/metrics_${DATE}.json

          # 最新のメトリクスファイルも更新
          cp quality-metrics.json quality-history/latest-metrics.json

      - name: Compare with baseline
        if: github.event_name == 'pull_request'
        run: |
          echo "📊 ベースラインとの比較中..."

          # ベースブランチをチェックアウト
          git checkout ${{ github.event.pull_request.base.sha }}

          # ベースブランチでメトリクス収集（エラーは無視）
          uv run python scripts/coverage-monitor.py --generate-report || true
          uv run ruff check . --output-format=json > base-ruff-report.json || true
          uv run mypy src/ --json-report base-mypy-report.json || true
          uv run pytest tests/ --json-report --json-report-file=base-test-report.json --cov=src/setup_repo --cov-report=json:base-coverage.json || true

          # 現在のブランチに戻る
          git checkout ${{ github.event.pull_request.head.sha }}

          # 比較レポートを生成
          uv run python -c "
          import json
          import os

          def load_json_safe(filename):
              try:
                  with open(filename, 'r') as f:
                      return json.load(f)
              except:
                  return {}

          # 現在とベースのメトリクスを読み込み
          current = load_json_safe('quality-metrics.json')

          # ベースのメトリクスを生成
          base_metrics = {
              'ruff_issues': 0,
              'mypy_errors': 0,
              'test_coverage': 0.0
          }

          base_ruff = load_json_safe('base-ruff-report.json')
          if isinstance(base_ruff, list):
              base_metrics['ruff_issues'] = len(base_ruff)

          base_mypy = load_json_safe('base-mypy-report.json')
          base_metrics['mypy_errors'] = len(base_mypy.get('errors', []))

          base_cov = load_json_safe('base-coverage.json')
          if 'totals' in base_cov:
              base_metrics['test_coverage'] = base_cov['totals']['percent_covered']

          # 比較結果を生成
          comparison = {
              'coverage_diff': current.get('test_coverage', 0) - base_metrics['test_coverage'],
              'ruff_diff': current.get('ruff_issues', 0) - base_metrics['ruff_issues'],
              'mypy_diff': current.get('mypy_errors', 0) - base_metrics['mypy_errors']
          }

          # 比較レポートをMarkdown形式で生成
          with open('quality-comparison.md', 'w') as f:
              f.write('# 📊 品質メトリクス比較\\n\\n')
              f.write('## ベースブランチとの比較\\n\\n')

              # カバレッジ比較
              cov_diff = comparison['coverage_diff']
              cov_icon = '✅' if cov_diff >= 0 else '⚠️' if cov_diff > -5 else '❌'
              f.write(f'{cov_icon} **カバレッジ**: {cov_diff:+.2f}% ({current.get(\"test_coverage\", 0):.2f}% vs {base_metrics[\"test_coverage\"]:.2f}%)\\n')

              # Ruff問題比較
              ruff_diff = comparison['ruff_diff']
              ruff_icon = '✅' if ruff_diff <= 0 else '⚠️' if ruff_diff <= 5 else '❌'
              f.write(f'{ruff_icon} **Ruff問題**: {ruff_diff:+d} 件 ({current.get(\"ruff_issues\", 0)} vs {base_metrics[\"ruff_issues\"]})\\n')

              # MyPyエラー比較
              mypy_diff = comparison['mypy_diff']
              mypy_icon = '✅' if mypy_diff <= 0 else '⚠️' if mypy_diff <= 3 else '❌'
              f.write(f'{mypy_icon} **MyPyエラー**: {mypy_diff:+d} 件 ({current.get(\"mypy_errors\", 0)} vs {base_metrics[\"mypy_errors\"]})\\n\\n')

              # 総合判定
              if cov_diff >= 0 and ruff_diff <= 0 and mypy_diff <= 0:
                  f.write('✅ **総合判定**: 品質が向上または維持されています\\n')
              elif cov_diff >= -2 and ruff_diff <= 5 and mypy_diff <= 3:
                  f.write('⚠️ **総合判定**: 軽微な品質低下があります\\n')
              else:
                  f.write('❌ **総合判定**: 品質が大幅に低下しています\\n')

          print('品質比較レポートを生成しました')
          "

      - name: Upload quality artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: quality-metrics
          path: |
            quality-metrics.json
            quality-summary.md
            quality-comparison.md
            quality-history/
            ruff-report.json
            mypy-report.json
            test-report.json
          retention-days: 90

      - name: Publish quality summary
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            try {
              let summary = '';

              // 品質サマリーを読み込み
              if (fs.existsSync('quality-summary.md')) {
                summary += fs.readFileSync('quality-summary.md', 'utf8') + '\\n\\n';
              }

              // 比較レポートを読み込み
              if (fs.existsSync('quality-comparison.md')) {
                summary += fs.readFileSync('quality-comparison.md', 'utf8');
              }

              if (summary) {
                github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: summary
                });
              }
            } catch (error) {
              console.log('品質サマリーの投稿に失敗:', error);
            }

      - name: Check quality gates
        run: |
          echo "🚪 品質ゲートチェック中..."

          # 品質メトリクスを読み込み
          COVERAGE=$(uv run python -c "import json; data=json.load(open('quality-metrics.json')); print(data['test_coverage'])")
          RUFF_ISSUES=$(uv run python -c "import json; data=json.load(open('quality-metrics.json')); print(data['ruff_issues'])")
          MYPY_ERRORS=$(uv run python -c "import json; data=json.load(open('quality-metrics.json')); print(data['mypy_errors'])")
          TEST_FAILED=$(uv run python -c "import json; data=json.load(open('quality-metrics.json')); print(data['test_failed'])")

          echo "品質メトリクス:"
          echo "  カバレッジ: ${COVERAGE}%"
          echo "  Ruff問題: ${RUFF_ISSUES}件"
          echo "  MyPyエラー: ${MYPY_ERRORS}件"
          echo "  テスト失敗: ${TEST_FAILED}件"

          # 品質ゲートの判定
          GATE_PASSED=true

          if (( $(echo "$COVERAGE < 80" | bc -l) )); then
            echo "❌ カバレッジが80%を下回っています: ${COVERAGE}%"
            GATE_PASSED=false
          fi

          if [ "$TEST_FAILED" -gt 0 ]; then
            echo "❌ テストが失敗しています: ${TEST_FAILED}件"
            GATE_PASSED=false
          fi

          if [ "$RUFF_ISSUES" -gt 10 ]; then
            echo "⚠️ Ruff問題が多すぎます: ${RUFF_ISSUES}件"
          fi

          if [ "$MYPY_ERRORS" -gt 5 ]; then
            echo "⚠️ MyPyエラーが多すぎます: ${MYPY_ERRORS}件"
          fi

          if [ "$GATE_PASSED" = true ]; then
            echo "✅ 品質ゲートを通過しました"
          else
            echo "❌ 品質ゲートを通過できませんでした"
            exit 1
          fi
