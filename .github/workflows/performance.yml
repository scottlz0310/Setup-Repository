name: Performance Tests

run-name: "Performance Tests - Manual (${{ inputs.test_scale || 'medium' }})"

on:
  workflow_dispatch:
    inputs:
      test_scale:
        description: 'ãƒ†ã‚¹ãƒˆã‚¹ã‚±ãƒ¼ãƒ« (small/medium/large/all)'
        required: false
        default: 'medium'
        type: choice
        options:
          - small
          - medium
          - large
          - all

jobs:
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30

    strategy:
      matrix:
        python-version: ["3.11"]  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã¯ä»£è¡¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ã¿
      fail-fast: false

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          version: "latest"

      - name: Set up Python ${{ matrix.python-version }}
        run: uv python install ${{ matrix.python-version }}

      - name: Create virtual environment
        run: uv venv --python ${{ matrix.python-version }}

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
            .pytest_cache
          key: ${{ runner.os }}-perf-${{ matrix.python-version }}-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-perf-${{ matrix.python-version }}-
            ${{ runner.os }}-perf-

      - name: Install dependencies
        run: |
          uv sync --dev
          uv pip install psutil  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šç”¨

      - name: Check performance tests directory
        run: |
          if [ ! -d "tests/performance" ]; then
          echo "âš ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
          echo "SKIP_PERFORMANCE_TESTS=true" >> $GITHUB_ENV
          fi

      - name: Run small scale performance tests
        # yamllint disable-next-line rule:line-length
        if: env.SKIP_PERFORMANCE_TESTS != 'true' && (github.event.inputs.test_scale == 'small' || github.event.inputs.test_scale == 'all' || github.event.inputs.test_scale == '')
        run: |
          uv run pytest tests/performance/ \
          -m "performance and not slow" \
          -v \
          --tb=short \
          --durations=10 \
          --json-report \
          --json-report-file=performance-small.json \
          --cov-fail-under=0

      - name: Run medium scale performance tests
        # yamllint disable-next-line rule:line-length
        if: env.SKIP_PERFORMANCE_TESTS != 'true' && (github.event.inputs.test_scale == 'medium' || github.event.inputs.test_scale == 'all' || github.event.inputs.test_scale == '')
        run: |
          # yamllint disable-next-line rule:line-length
          uv run pytest tests/performance/test_sync_performance.py::TestSyncPerformance::test_medium_repository_set_performance \
          -v \
          --tb=short \
          --durations=10 \
          --json-report \
          --json-report-file=performance-medium.json \
          --cov-fail-under=0

      - name: Run large scale performance tests
        # yamllint disable-next-line rule:line-length
        if: env.SKIP_PERFORMANCE_TESTS != 'true' && (github.event.inputs.test_scale == 'large' || github.event.inputs.test_scale == 'all')
        run: |
          # yamllint disable-next-line rule:line-length
          uv run pytest tests/performance/test_sync_performance.py::TestSyncPerformance::test_large_repository_set_performance \
          -v \
          --tb=short \
          --durations=10 \
          --json-report \
          --json-report-file=performance-large.json \
          --cov-fail-under=0

      - name: Run concurrent operations performance tests
        if: env.SKIP_PERFORMANCE_TESTS != 'true'
        run: |
          # yamllint disable-next-line rule:line-length
          uv run pytest tests/performance/test_sync_performance.py::TestSyncPerformance::test_concurrent_operations_performance \
          -v \
          --tb=short \
          --durations=10 \
          --json-report \
          --json-report-file=performance-concurrent.json \
          --cov-fail-under=0

      - name: Generate performance report
        if: always()
        run: |
          echo "# ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœ" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f performance-small.json ]; then
            echo "## å°è¦æ¨¡ãƒ†ã‚¹ãƒˆçµæœ" >> $GITHUB_STEP_SUMMARY
            uv run python -c "
          import json
          try:
              with open('performance-small.json', 'r') as f:
                  data = json.load(f)
              print(f'- ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“: {data.get(\"duration\", \"N/A\")}ç§’')
              print(f'- æˆåŠŸãƒ†ã‚¹ãƒˆæ•°: {data.get(\"summary\", {}).get(\"passed\", 0)}')
              print(f'- å¤±æ•—ãƒ†ã‚¹ãƒˆæ•°: {data.get(\"summary\", {}).get(\"failed\", 0)}')
          except Exception as e:
              print(f'ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}')
          " >> $GITHUB_STEP_SUMMARY
          fi

          if [ -f performance-medium.json ]; then
            echo "## ä¸­è¦æ¨¡ãƒ†ã‚¹ãƒˆçµæœ" >> $GITHUB_STEP_SUMMARY
            uv run python -c "
          import json
          try:
              with open('performance-medium.json', 'r') as f:
                  data = json.load(f)
              print(f'- ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“: {data.get(\"duration\", \"N/A\")}ç§’')
              print(f'- æˆåŠŸãƒ†ã‚¹ãƒˆæ•°: {data.get(\"summary\", {}).get(\"passed\", 0)}')
              print(f'- å¤±æ•—ãƒ†ã‚¹ãƒˆæ•°: {data.get(\"summary\", {}).get(\"failed\", 0)}')
          except Exception as e:
              print(f'ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}')
          " >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload performance test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results-${{ matrix.python-version }}
          path: |
            performance-*.json
          retention-days: 30

      - name: Performance regression check
        run: |
          echo "ğŸ” ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œä¸­..."

          # åŸºæº–å€¤ã¨ã®æ¯”è¼ƒï¼ˆå°†æ¥çš„ã«ã¯ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã¨æ¯”è¼ƒï¼‰
          uv run python -c "
          import json
          import sys

          def check_performance_regression():
            try:
                # ä¸­è¦æ¨¡ãƒ†ã‚¹ãƒˆã®çµæœã‚’ãƒã‚§ãƒƒã‚¯
                if not os.path.exists('performance-medium.json'):
                    print('âš ï¸ ä¸­è¦æ¨¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
                    return

                with open('performance-medium.json', 'r') as f:
                    data = json.load(f)

                duration = data.get('duration', 0)
                failed_tests = data.get('summary', {}).get('failed', 0)

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–å€¤ï¼ˆç§’ï¼‰
                MAX_DURATION = 25.0

                if duration > MAX_DURATION:
                    print(f'âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°æ¤œå‡º: å®Ÿè¡Œæ™‚é–“ {duration}ç§’ > åŸºæº–å€¤ {MAX_DURATION}ç§’')
                    sys.exit(1)

                if failed_tests > 0:
                    print(f'âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå¤±æ•—: {failed_tests}ä»¶')
                    sys.exit(1)

                print(f'âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–ã‚¯ãƒªã‚¢: å®Ÿè¡Œæ™‚é–“ {duration}ç§’ <= åŸºæº–å€¤ {MAX_DURATION}ç§’')

            except Exception as e:
                print(f'âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}')
                import traceback
                traceback.print_exc()
                sys.exit(1)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å¤±æ•—ã•ã›ã‚‹

          import os
          check_performance_regression()
          "
