name: Performance Tests

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # æ¯æ—¥åˆå‰2æ™‚ï¼ˆUTCï¼‰ã«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_scale:
        description: 'ãƒ†ã‚¹ãƒˆã‚¹ã‚±ãƒ¼ãƒ« (small/medium/large/all)'
        required: false
        default: 'medium'
        type: choice
        options:
        - small
        - medium
        - large
        - all

jobs:
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30

    strategy:
      matrix:
        python-version: ["3.11"]  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã¯ä»£è¡¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ã¿
      fail-fast: false

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"

    - name: Set up Python ${{ matrix.python-version }}
      run: uv python install ${{ matrix.python-version }}

    - name: Create virtual environment
      run: uv venv --python ${{ matrix.python-version }}

    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/uv
          .venv
          .pytest_cache
        key: ${{ runner.os }}-perf-${{ matrix.python-version }}-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-perf-${{ matrix.python-version }}-
          ${{ runner.os }}-perf-

    - name: Install dependencies
      run: |
        uv sync --dev
        uv pip install psutil  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šç”¨

    - name: Run small scale performance tests
      if: github.event.inputs.test_scale == 'small' || github.event.inputs.test_scale == 'all' || github.event.inputs.test_scale == ''
      run: |
        uv run pytest tests/performance/ \
          -m "performance and not slow" \
          -v \
          --tb=short \
          --durations=10 \
          --json-report \
          --json-report-file=performance-small.json

    - name: Run medium scale performance tests
      if: github.event.inputs.test_scale == 'medium' || github.event.inputs.test_scale == 'all' || github.event.inputs.test_scale == ''
      run: |
        uv run pytest tests/performance/test_sync_performance.py::TestSyncPerformance::test_medium_repository_set_performance \
          -v \
          --tb=short \
          --durations=10 \
          --json-report \
          --json-report-file=performance-medium.json

    - name: Run large scale performance tests
      if: github.event.inputs.test_scale == 'large' || github.event.inputs.test_scale == 'all'
      run: |
        uv run pytest tests/performance/test_sync_performance.py::TestSyncPerformance::test_large_repository_set_performance \
          -v \
          --tb=short \
          --durations=10 \
          --json-report \
          --json-report-file=performance-large.json

    - name: Run concurrent operations performance tests
      run: |
        uv run pytest tests/performance/test_sync_performance.py::TestSyncPerformance::test_concurrent_operations_performance \
          -v \
          --tb=short \
          --durations=10 \
          --json-report \
          --json-report-file=performance-concurrent.json

    - name: Generate performance report
      if: always()
      run: |
        echo "# ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœ" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ -f performance-small.json ]; then
          echo "## å°è¦æ¨¡ãƒ†ã‚¹ãƒˆçµæœ" >> $GITHUB_STEP_SUMMARY
          uv run python -c "
        import json
        try:
            with open('performance-small.json', 'r') as f:
                data = json.load(f)
            print(f'- ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“: {data.get(\"duration\", \"N/A\")}ç§’')
            print(f'- æˆåŠŸãƒ†ã‚¹ãƒˆæ•°: {data.get(\"summary\", {}).get(\"passed\", 0)}')
            print(f'- å¤±æ•—ãƒ†ã‚¹ãƒˆæ•°: {data.get(\"summary\", {}).get(\"failed\", 0)}')
        except Exception as e:
            print(f'ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}')
        " >> $GITHUB_STEP_SUMMARY
        fi

        if [ -f performance-medium.json ]; then
          echo "## ä¸­è¦æ¨¡ãƒ†ã‚¹ãƒˆçµæœ" >> $GITHUB_STEP_SUMMARY
          uv run python -c "
        import json
        try:
            with open('performance-medium.json', 'r') as f:
                data = json.load(f)
            print(f'- ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“: {data.get(\"duration\", \"N/A\")}ç§’')
            print(f'- æˆåŠŸãƒ†ã‚¹ãƒˆæ•°: {data.get(\"summary\", {}).get(\"passed\", 0)}')
            print(f'- å¤±æ•—ãƒ†ã‚¹ãƒˆæ•°: {data.get(\"summary\", {}).get(\"failed\", 0)}')
        except Exception as e:
            print(f'ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}')
        " >> $GITHUB_STEP_SUMMARY
        fi

    - name: Upload performance test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-test-results-${{ matrix.python-version }}
        path: |
          performance-*.json
        retention-days: 30

    - name: Performance regression check
      if: github.event_name == 'pull_request'
      run: |
        echo "ğŸ” ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œä¸­..."

        # åŸºæº–å€¤ã¨ã®æ¯”è¼ƒï¼ˆå°†æ¥çš„ã«ã¯ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã¨æ¯”è¼ƒï¼‰
        uv run python -c "
        import json
        import sys

        def check_performance_regression():
            try:
                # ä¸­è¦æ¨¡ãƒ†ã‚¹ãƒˆã®çµæœã‚’ãƒã‚§ãƒƒã‚¯
                if not os.path.exists('performance-medium.json'):
                    print('âš ï¸ ä¸­è¦æ¨¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
                    return

                with open('performance-medium.json', 'r') as f:
                    data = json.load(f)

                duration = data.get('duration', 0)
                failed_tests = data.get('summary', {}).get('failed', 0)

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–å€¤ï¼ˆç§’ï¼‰
                MAX_DURATION = 25.0

                if duration > MAX_DURATION:
                    print(f'âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°æ¤œå‡º: å®Ÿè¡Œæ™‚é–“ {duration}ç§’ > åŸºæº–å€¤ {MAX_DURATION}ç§’')
                    sys.exit(1)

                if failed_tests > 0:
                    print(f'âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå¤±æ•—: {failed_tests}ä»¶')
                    sys.exit(1)

                print(f'âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–ã‚¯ãƒªã‚¢: å®Ÿè¡Œæ™‚é–“ {duration}ç§’ <= åŸºæº–å€¤ {MAX_DURATION}ç§’')

            except Exception as e:
                print(f'âš ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}')
                # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯è­¦å‘Šã®ã¿ã§ãƒ“ãƒ«ãƒ‰ã¯ç¶™ç¶š

        import os
        check_performance_regression()
        "

  benchmark-comparison:
    name: Benchmark Comparison
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    needs: performance-tests

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # å±¥æ­´å…¨ä½“ã‚’å–å¾—

    - name: Download performance results
      uses: actions/download-artifact@v4
      with:
        name: performance-test-results-3.11
        path: ./current-results

    - name: Compare with baseline
      run: |
        echo "# ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "ç¾åœ¨ã®PRã¨ main ãƒ–ãƒ©ãƒ³ãƒã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # å°†æ¥çš„ã«ã¯ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®è©³ç´°æ¯”è¼ƒã‚’å®Ÿè£…
        echo "âš ï¸ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒæ©Ÿèƒ½ã¯é–‹ç™ºä¸­ã§ã™ã€‚" >> $GITHUB_STEP_SUMMARY
        echo "ç¾åœ¨ã¯çµ¶å¯¾å€¤ã§ã®åŸºæº–ãƒã‚§ãƒƒã‚¯ã®ã¿å®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã™ã€‚" >> $GITHUB_STEP_SUMMARY

    - name: Comment PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');

          try {
            let comment = '## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœ\n\n';

            // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            const files = ['performance-medium.json', 'performance-concurrent.json'];

            for (const file of files) {
              const filePath = `./current-results/${file}`;
              if (fs.existsSync(filePath)) {
                const data = JSON.parse(fs.readFileSync(filePath, 'utf8'));
                const testType = file.replace('performance-', '').replace('.json', '');

                comment += `### ${testType} ãƒ†ã‚¹ãƒˆ\n`;
                comment += `- å®Ÿè¡Œæ™‚é–“: ${data.duration || 'N/A'}ç§’\n`;
                comment += `- æˆåŠŸ: ${data.summary?.passed || 0}ä»¶\n`;
                comment += `- å¤±æ•—: ${data.summary?.failed || 0}ä»¶\n\n`;
              }
            }

            comment += 'è©³ç´°ãªçµæœã¯ Artifacts ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚\n';

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          } catch (error) {
            console.log('ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµæœã®ã‚³ãƒ¡ãƒ³ãƒˆä½œæˆã«å¤±æ•—:', error);
          }
